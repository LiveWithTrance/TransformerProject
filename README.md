# Web Scrape Plugin for ChatGPT

## Getting Started

### Prerequisites

- Python 3.x
- Libraries: `httpx`, `requests`, `beautifulsoup4`, `openai`, `streamlit`
- OpenAI API key

### Installation

Provide instructions for setting up the project environment. Include steps to install necessary Python libraries and how to set up the OpenAI API key.

## HuggingFace Space

https://huggingface.co/spaces/Jackeylove/WebScrape

## Code Demonstration

Refer to Jupyter notebook demonstration.ipynb


## Web Scraping Policy

Please always refer to website's terms of service and robots.txt file

https://www.termsfeed.com/blog/web-scraping-laws/

https://webbiquity.com/marketing-technology/the-etiquette-of-web-scraping-and-how-to-use-web-scraping-legally/

## Project Overview

Since OpenAI release ChatGPT's plugin store. There have been thousands of plugins coming out everyday including Web Scrape plugins. ChatGPT also lauched its own Web Scrape Plugin using Microsoft Bing and it is very powerful. To better understand how ChatGPT could search online. I decided to develop a toy web scrape plugin using Python and ChatGPT's api.

This project is trying to develop a Web Scrape Plugin with the help of ChatGPT. The core thought is providing the scraping content to ChatGPT and get ChatGPT's feedback iteratively to improve the final scraping outcome.

This project is using GPT-3.5-turbo. (You can use GPT-4 but the price is higher)

### Comparance Between GPT-3.5-turbo without Web Scraping Plugin

**Generated by GPT-3.5-turbo:**

<img width="751" alt="截屏2023-11-14 23 25 21" src="https://github.com/LiveWithTrance/TransformerProject/assets/111295481/22bd8297-622c-4edf-adeb-9fc4325ba482">

**Generated by GPT-3.5-turbo with Web Scrape Plugin:**

<img width="835" alt="截屏2023-11-14 23 25 47" src="https://github.com/LiveWithTrance/TransformerProject/assets/111295481/1d9e33d5-2893-4600-a18c-1912fb3aaafa">

**Generated by GPT-3.5-turbo:**

<img width="787" alt="截屏2023-11-14 23 42 17" src="https://github.com/LiveWithTrance/TransformerProject/assets/111295481/e2f4bbe5-0d4f-4e0a-91d1-e12eca7f462b">

**Generated by GPT-3.5-turbo with Web Scrape Plugin:**

<img width="884" alt="截屏2023-11-14 23 41 19" src="https://github.com/LiveWithTrance/TransformerProject/assets/111295481/692fb8af-943e-4f64-9159-63beb2dd92ec">


### Brief introduction to do Web Scrape

**Step1.** Analynaze the structure of the websites and locate the information you want to web scrape.

**Step2.** Use programme to simulate user and fetch the raw code where you have located from the target website.

**Step3.** Extract the information you need from these raw code.

### Difficulties of Web Scraping for different websites

It might be easy to web scrape a single website because the structure of a single website is relatively similar and you can also easily foresee any changes of the struture by clicking links and analyzing the raw code within a website.

However, different websites might have different structures. A web scrape programme for a certain website cannot be applied to other different websites without adjustment. But thanks to ChatGPT, it makes more easy for us to do the web scraping more generally.



## Details of this web scraper

Description only, see details at Jupyter notebook demonstration.ipynb

**Step1.** Search the input on Google, get top 10 links 

**Step2.** Extract information at where the most of websites will put their information (Here I choose h1,h2,h3 as configs, can be modified)

**Step3.** Ask ChatGPT to extract the information and analyze if it is enough to answer the input question. If so, end web scraping. If not, keep web scraping until ChatGPT get enough information to answer the input question.

**Step4.** Give the outcome of the web scrape as reference to ChatGPT and generate final output.


## How to avoid anti-webscrape from websites

I encountered multiple HTTP errors because of different websites‘ anti-webscraping measures. The following are some of the measures could be used to avoid the anti-webscraping. 

Please always remember to check the websites' policy before you do web scraping and whether the following measures are legal or not.

### Do not scrape too fast

rapid scraping can easily trigger a website's rate-limiting mechanisms. Implement a delay between your requests to mimic human browsing speeds. Use Python's time.sleep() function to add pauses in your scraping loop. 

### Use Cookie to avoid loginin

Some websites require you to log in or maintain a session to access certain data. In such cases, using cookies can help maintain your session state across requests. This can be achieved by utilizing requests.Session in Python, which will handle cookies for you:

import requests

with requests.Session() as session:
    # Perform login or initial request
    # Follow-up requests will use the same session

### IP pool

Using a pool of different IP addresses can help you avoid IP bans. This can be done through proxies. Rotating your IP address with each request or at regular intervals can significantly reduce the risk of being blocked.

### User-Agent Rotation

Websites can track if multiple requests are coming from the same user-agent. Rotate user-agent strings to reduce the risk of detection.

### Utilize Official APIs

Whenever possible, use official APIs provided by the website. For example, Google's Custom Search JSON API allows you to perform searches and retrieve results in a structured format. APIs are designed to handle requests and are a legitimate way to access data.


## Problems need to be solved

### Low effiency 

This web scrape plugin works very slow. The reason might be this programme spend much time on letting ChatGPT to help analyze the raw code. Also, the structure of the code might also need to be improved.

### Huge cost of tokens

Durning the analysis of the raw code, there are much useless information. Also, the iteration of letting ChatGPT analyze the content of web scrape will also cost many tokens. 

### Websites policy

This programme does not contain analyzing the policies of different websites. I did this maually. But I might miss some of the policied by mistakes. So there might be potential risks.

## How does ChatGPT web scrape?

Durning comparing the performance and power between my web scraping programme 

**Me:**

Could you please explain how you interact with websites?

**ChatGPT:**

Certainly! My interaction with websites is designed to simulate how a human user would browse the internet using a standard web browser. Here's a breakdown of how it works:

Search and Navigation: I can issue search queries to a search engine, similar to how you might use Google or another search engine. Once the search results are returned, I can select a result to view, much like clicking on a link in a web browser.

Reading and Quoting Information: I can read and understand the text presented on the web pages I navigate to. When I find relevant information, I can quote specific portions of text. This is akin to highlighting or noting down important information from a webpage.

Back and Forward Navigation: Like using the back and forward buttons in a browser, I can return to previous pages I've viewed or move forward again.

Scrolling: I can scroll up or down on a page to view more content, similar to how you might scroll through a long article or search results.

Opening Specific URLs: If provided with a specific URL, I can open it directly.

However, there are some limitations:

No Interaction with Dynamic Content: I can't interact with dynamic content such as forms, dropdowns, or any interactive elements on a webpage.

No Raw HTML Processing: I don't read or interpret the raw HTML, JavaScript, or CSS code of the web pages. My interactions are based solely on the visible text and information.
Privacy and Security: My browsing capability is designed with privacy and security in mind. I can't perform transactions, fill out forms, or perform any actions that would involve personal data or security-sensitive operations.

This system allows me to access a wide range of information available on the public internet, similar to a human researcher using a web browser, but within the defined constraints to ensure privacy and compliance with data handling standards.

**Me:**

So you never read the raw data to do web searching?

**ChatGPT:**

That's correct, I do not read or process the raw HTML data of web pages for web searching or any other purpose. My interactions with websites are based on the visible content and text displayed on the pages, much like how a human user would view and interpret information using a standard web browser.

When I perform searches or navigate web pages, I'm using the textual information presented on the screen, not the underlying HTML, CSS, JavaScript, or other code that constitutes the webpage. This approach aligns with user privacy and data security principles, ensuring that I operate within a safe and controlled browsing environment.



## Next Steps
### 1. **Implement Advanced Error Handling**
   - Broaden error handling to manage different types of exceptions more effectively.
   - Implement retries with exponential backoff for transient errors.
   - Log errors for debugging and continuous improvement of the scraper.

### 2. **Optimize Request Throttling**
   - Dynamically adjust request intervals based on server response or specific site’s rate-limiting policies.
   - Implement adaptive throttling that can learn and adjust based on past scraping experiences.

### 3. **Implement a More Robust Parsing Strategy**
   - Make the HTML parsing more resilient to changes in the website's structure.
   - Use machine learning or heuristics to identify and extract relevant data even when the HTML structure changes.

### 4. **Data Quality Checks**
   - Implement validation checks to ensure the integrity and accuracy of the scraped data.
   - Regularly test the scraper against changes in website layout or content.

### 5. **Legal and Compliance Review**
   - Regularly review legal compliance, especially if scraping websites in different jurisdictions.
   - Stay updated with the latest laws and regulations regarding web scraping and data privacy.

### 6. **User Feedback Integration**
   - If the scraper is part of a larger application, implement user feedback mechanisms to identify and rectify issues quickly.

### 7. **Performance Optimization**
   - Profile the program to identify bottlenecks.
   - Optimize the code for better performance, particularly if dealing with large-scale data.

## Overview Recording

Overview Recording will be uploaded soon.

## Useful Links
- [Web Scraping Laws and Ethics](https://www.termsfeed.com/blog/web-scraping-laws/)
- [The Etiquette of Web Scraping](https://webbiquity.com/marketing-technology/the-etiquette-of-web-scraping-and-how-to-use-web-scraping-legally/)
- [OpenAI API Documentation](https://beta.openai.com/docs/)
- [OpenAI API Use](https://platform.openai.com/docs/api-reference)
- [A Web Scraping Methodology for Bypassing Twitter API Restrictions](https://arxiv.org/abs/1803.09875)
- [Should we trust web scraped data?](https://arxiv.org/pdf/2308.02231.pdf)
- [What is Web Scraping and How to Use It?](https://www.geeksforgeeks.org/what-is-web-scraping-and-how-to-use-it/)
- [Is Web Scraping Legal? Ethical Web Scraping Guide in 2023](https://research.aimultiple.com/web-scraping-legal)
- [Training AI on Personal Data Scraped from the Web](https://iapp.org/news/a/training-ai-on-personal-data-scraped-from-the-web)
- [A Comprehensive Legal Guide to Web Scraping in the US](https://www.mccarthylg.com/a-comprehensive-legal-guide-to-web-scraping-in-the-us)
- [The Etiquette of Web Scraping and How to Use Web Scraping Legally](https://webbiquity.com/marketing-technology/the-etiquette-of-web-scraping-and-how-to-use-web-scraping-legally)
- [API Reference - OpenAI API](https://platform.openai.com/docs/api-reference)



