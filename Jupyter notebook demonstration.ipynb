{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Web Scrape Via Google\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "r8OE7TGk6sm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzJ_m5K-qOm0",
        "outputId": "c192d91d-e24f-418a-bfc9-6b50cc2d2230"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from httpx import Response\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "from openai import OpenAI  # Assuming OpenAI's GPT is being used\n",
        "import streamlit as st\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key='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'\n",
        "    )\n",
        "def analyze_with_chatgpt(keyword,content):\n",
        "    \"\"\"\n",
        "    Send the scraped content to the ChatGPT API for analysis.\n",
        "    Returns a decision on whether more scraping is needed.\n",
        "    \"\"\"\n",
        "\n",
        "    content = content[:1500]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Here is the outcome of {keyword} by web scraping \\\n",
        "            , analyze the content: {content}. Does this content contain enough information about {keyword}? Your answer should just be YES or NO, don't contain any other words than YES or No\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_with_chatgpt(keyword, content):\n",
        "    \"\"\"\n",
        "    Extract useful information from final data\n",
        "    \"\"\"\n",
        "\n",
        "    content=str(content)\n",
        "    content=content[:1500]\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Here is all the content about {keyword} from web scrape, please extract important information and summarize these content: {content}. All you output should be in paragraphs\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def google_search(query):\n",
        "    # This is a placeholder. In a real-world application, use an API.\n",
        "    search_url = f\"https://www.google.com/search?q={query}\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(search_url, headers=headers)\n",
        "    return response.text\n",
        "def extract_urls_from_google(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    links = soup.find_all('a')\n",
        "    urls = []\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if href and \"url?q=\" in href and not \"google.com\" in href:\n",
        "            actual_url = href.split(\"url?q=\")[1].split(\"&sa=U\")[0]\n",
        "            urls.append(actual_url)\n",
        "    urls=urls[:3] # fetch top 4 websites\n",
        "\n",
        "    return urls\n",
        "\n",
        "\n",
        "def fetch_html(url):\n",
        "    try:\n",
        "        # Random delay between requests to reduce the chance of being blocked\n",
        "        time.sleep(0.3)  # Wait 1 to 5 seconds\n",
        "        headers = {'User-Agent': 'Mozilla/5.0'}  # Consider rotating user agents\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.HTTPError as e:\n",
        "        print(f\"HTTP Error: {e} for url: {url}\")\n",
        "        return None\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def scrape_website(url, config):\n",
        "    html = fetch_html(url)\n",
        "    if not html:\n",
        "        return \"Failed to fetch webpage\"\n",
        "\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    scraped_data = {}\n",
        "\n",
        "    for key, value in config.items():\n",
        "        elements = soup.select(value)\n",
        "        scraped_data[key] = [element.get_text().strip() for element in elements]\n",
        "\n",
        "    return scraped_data\n",
        "\n",
        "\n",
        "def main(keyword, config):\n",
        "    search_results_html = google_search(keyword)\n",
        "    urls = extract_urls_from_google(search_results_html)\n",
        "    all_scraped_data = []\n",
        "    for url in urls:\n",
        "        scraped_data = scrape_website(url, config)\n",
        "        if isinstance(scraped_data, dict):  # Check if scraped_data is a dictionary\n",
        "            content_string = \" \".join([\" \".join(values) for values in scraped_data.values()])\n",
        "            all_scraped_data.append(scraped_data)\n",
        "\n",
        "            decision = analyze_with_chatgpt(keyword,content_string)\n",
        "            if not decision.lower() in ['yes','no']:\n",
        "               print('Web Scrape occur error, please try again')\n",
        "               break\n",
        "\n",
        "            if decision.lower()=='yes':\n",
        "                break  # Break if no more information is needed\n",
        "            all_scraped_data =[]\n",
        "        else:\n",
        "            print(f\"Failed to scrape {url}\")\n",
        "\n",
        "    final_data = extract_with_chatgpt(keyword,scraped_data)\n",
        "    return final_data\n",
        "# Example usage\n",
        "\n",
        "keyword = \"information about Data Science Institute of Vanderbilt\"\n",
        "config = {\n",
        "    \"titles\": \"h1, h2, h3\",\n",
        "    \"paragraphs\": \"p\",\n",
        "}\n",
        "print(main(keyword, config))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diFn7NYhpwsD",
        "outputId": "52c1d38a-122d-4098-8555-9bb7ab20fd48"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Data Science Institute of Vanderbilt offers a Masters in Data Science program. This program provides students with access to world-class faculty, strong industry connections, and an innovative curriculum that blends statistical principles with computational progressiveness. It is designed to be accessible to students from a variety of educational backgrounds.\n",
            "\n",
            "Located in Nashville, one of the fastest-growing cities in the United States, Vanderbilt allows students to live and learn in a vibrant and thriving community. Nashville is ranked as the 3rd best city for young professionals and offers numerous opportunities for data scientists. The data science job market in Middle Tennessee is growing rapidly, with a strong community built around healthcare, entertainment, and technology.\n",
            "\n",
            "The program is looking for students with unique academic backgrounds and computational experiences. The curriculum combines computation, data analysis, and ethics to produce well-rounded data scientists. The goal is to equip students with the necessary skills and knowledge to succeed in the field.\n",
            "\n",
            "The Data Science Institute of Vanderbilt also hosts various recruitment events for prospective students to learn more about the program. These events provide an opportunity to engage with faculty and gather more information about the program and its offerings.\n",
            "\n",
            "For those interested in applying, the institute provides resources and support for the application process. Information about admissions, tuition, fees, and scholarships can be found on their website. The institute also offers career resources and support to aid students in their professional development.\n",
            "\n",
            "In conclusion, the Data Science Institute of Vanderbilt offers a comprehensive and innovative Masters in Data Science program. With its strong faculty, industry connections, and unique curriculum, the institute aims to prepare students for successful careers in data science. The growing data science job market in Nashville and Middle Tennessee further enhances the opportunities available to graduates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(st.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PigkUuvBDr7",
        "outputId": "e6e8953b-5a88-4fd3-d4d8-84b8f98c0239"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method text in module streamlit.delta_generator:\n",
            "\n",
            "text(body: 'SupportsStr', *, help: Optional[str] = None) -> 'DeltaGenerator' method of streamlit.delta_generator.DeltaGenerator instance\n",
            "    Write fixed-width and preformatted text.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    body : str\n",
            "        The string to display.\n",
            "    \n",
            "    help : str\n",
            "        An optional tooltip that gets displayed next to the text.\n",
            "    \n",
            "    Example\n",
            "    -------\n",
            "    >>> import streamlit as st\n",
            "    >>>\n",
            "    >>> st.text('This is some text.')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Scrape For Specified Websites\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fkLqjkBR610T"
      }
    }
  ]
}
